{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Data - I\n",
    "\n",
    "\n",
    "There are multiple ways of getting data into python, depending on where the data is stored. The simplest case is when you have data in CSV files, but often, you need to get data from other formats, sources and documents, such as text files, relational databases, websites, APIs, PDF documents, etc. \n",
    "\n",
    "In the following sections, you will learn to get data into python from a number of sources. You will learn to:\n",
    "* Get data from text files\n",
    "* Get data from relational databases\n",
    "* Scrape data from websites\n",
    "* Get data from publicly available APIs\n",
    "* Read PDFs into python\n",
    "\n",
    "In the process, you will also learn how to deal with nuances that inevitably come while getting data from various sources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Delimited Files\n",
    "\n",
    "Delimited files are usually text files, where columns are separated by delimiters (such as commas, tabs, semicolons etc.) and each new line is a row.\n",
    "\n",
    "For instance, we have the ```companies.txt``` file, where each column is separated by a tab:\n",
    "\n",
    "<img src=\"companies.png\" style=\"height: 500px; width: 600px\">\n",
    "\n",
    "The easiest way to read delimited files is using ```pd.read_csv(filepath, sep, header)``` and specify a separator (delimiter).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xa0 in position 25: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xa0 in position 25: invalid start byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1deddf539dc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# reading companies file: throws an error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcompanies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"companies.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2035\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2037\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2038\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xa0 in position 25: invalid start byte"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# reading companies file: throws an error\n",
    "companies = pd.read_csv(\"companies.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error appears because of a decoding issue pandas is facing. The best way to resolve such problems to search for help, for example on stack overflow, where you'd get excellent suggestions from the community.\n",
    "\n",
    "One possible solution to this problem is found here: https://stackoverflow.com/questions/18171739/unicodedecodeerror-when-reading-csv-file-in-pandas-with-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permalink</th>\n",
       "      <th>name</th>\n",
       "      <th>homepage_url</th>\n",
       "      <th>category_list</th>\n",
       "      <th>status</th>\n",
       "      <th>country_code</th>\n",
       "      <th>state_code</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "      <th>founded_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Organization/-Fame</td>\n",
       "      <td>#fame</td>\n",
       "      <td>http://livfame.com</td>\n",
       "      <td>Media</td>\n",
       "      <td>operating</td>\n",
       "      <td>IND</td>\n",
       "      <td>16</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Organization/-Qounter</td>\n",
       "      <td>:Qounter</td>\n",
       "      <td>http://www.qounter.com</td>\n",
       "      <td>Application Platforms|Real Time|Social Network...</td>\n",
       "      <td>operating</td>\n",
       "      <td>USA</td>\n",
       "      <td>DE</td>\n",
       "      <td>DE - Other</td>\n",
       "      <td>Delaware City</td>\n",
       "      <td>04-09-2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Organization/-The-One-Of-Them-Inc-</td>\n",
       "      <td>(THE) ONE of THEM,Inc.</td>\n",
       "      <td>http://oneofthem.jp</td>\n",
       "      <td>Apps|Games|Mobile</td>\n",
       "      <td>operating</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Organization/0-6-Com</td>\n",
       "      <td>0-6.com</td>\n",
       "      <td>http://www.0-6.com</td>\n",
       "      <td>Curated Web</td>\n",
       "      <td>operating</td>\n",
       "      <td>CHN</td>\n",
       "      <td>22</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>01-01-2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Organization/004-Technologies</td>\n",
       "      <td>004 Technologies</td>\n",
       "      <td>http://004gmbh.de/en/004-interact</td>\n",
       "      <td>Software</td>\n",
       "      <td>operating</td>\n",
       "      <td>USA</td>\n",
       "      <td>IL</td>\n",
       "      <td>Springfield, Illinois</td>\n",
       "      <td>Champaign</td>\n",
       "      <td>01-01-2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             permalink                    name  \\\n",
       "0                  /Organization/-Fame                   #fame   \n",
       "1               /Organization/-Qounter                :Qounter   \n",
       "2  /Organization/-The-One-Of-Them-Inc-  (THE) ONE of THEM,Inc.   \n",
       "3                /Organization/0-6-Com                 0-6.com   \n",
       "4       /Organization/004-Technologies        004 Technologies   \n",
       "\n",
       "                        homepage_url  \\\n",
       "0                 http://livfame.com   \n",
       "1             http://www.qounter.com   \n",
       "2                http://oneofthem.jp   \n",
       "3                 http://www.0-6.com   \n",
       "4  http://004gmbh.de/en/004-interact   \n",
       "\n",
       "                                       category_list     status country_code  \\\n",
       "0                                              Media  operating          IND   \n",
       "1  Application Platforms|Real Time|Social Network...  operating          USA   \n",
       "2                                  Apps|Games|Mobile  operating          NaN   \n",
       "3                                        Curated Web  operating          CHN   \n",
       "4                                           Software  operating          USA   \n",
       "\n",
       "  state_code                 region           city  founded_at  \n",
       "0         16                 Mumbai         Mumbai         NaN  \n",
       "1         DE             DE - Other  Delaware City  04-09-2014  \n",
       "2        NaN                    NaN            NaN         NaN  \n",
       "3         22                Beijing        Beijing  01-01-2007  \n",
       "4         IL  Springfield, Illinois      Champaign  01-01-2010  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using encoding = \"ISO-8859-1\"\n",
    "companies = pd.read_csv(\"companies.txt\", sep=\"\\t\", encoding = \"ISO-8859-1\")\n",
    "companies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Data From Relational Databases\n",
    "\n",
    "Data is commonly stored in RDBMS, and it is easy to get it into Python. We'll use the most common one - MySQL.\n",
    "\n",
    "There are many libraries to connect MySQL and Python, such as pymysql, MySQLdb, etc. All of them follow the following procedure to connect to MySQL:\n",
    "- Create a connection object between MySQL and python\n",
    "- Create a cursor object (you use the cursor to open and close the connection) \n",
    "- Execute the SQL query \n",
    "- Retrive results of the query using methods such as ```fetchone()```, ```fetchall()```, etc.\n",
    "\n",
    "Let' work through an example using PyMySQL. You can install it using ```pip install pymysql```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "# create a connection object 'conn'\n",
    "conn = pymysql.connect(host=\"localhost\", # your host, localhost for your local machine\n",
    "                     user=\"root\", # your username, usually \"root\" for localhost\n",
    "                      passwd=\"jarvis\", # your password\n",
    "                      db=\"mysql\") # name of the data base; world comes inbuilt with mysql\n",
    "\n",
    "# create a cursor object c\n",
    "c = conn.cursor()\n",
    "\n",
    "# execute a query using c.execute\n",
    "c.execute(\"select * from db;\")\n",
    "\n",
    "# getting the first row of data as a tuple\n",
    "all_rows = c.fetchall()\n",
    "\n",
    "# to get only the first row, use c.fetchone() instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "(('localhost', 'performance_schema', 'mysql.session', 'Y', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N'), ('localhost', 'sys', 'mysql.sys', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'Y'))\n"
     ]
    }
   ],
   "source": [
    "# notice that it returns a tuple of tuples: each row is a tuple\n",
    "print(type(all_rows))\n",
    "\n",
    "# printing the first few rows\n",
    "print(all_rows[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it would be useful to convert the list into a dataframe, since you can now work with dataframes easily. In this case, you can use the ```pd.DataFrame()``` function, and pass the list version of the tuple.\n",
    "\n",
    "```pd.DataFrame(list_of_tuples)``` converts each tuple in the list to a row in the DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(all_rows), columns=[\"ID\", \"Name\", \"Country\", \"District\", \"Population\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Data From Websites\n",
    "\n",
    "Web scraping refers to the art of programmatically getting data from the internet.  One of the coolest features of python is that it makes it easy to scrape websites.\n",
    "\n",
    "In Python 3, the most popular library for web scraping is ```BeautifulSoup```. To use ```BeautifulSoup```, we will also need the ```requests``` module, which basically connects to a given URL and fetches data from it (in HTML format). A web page is basically HTML code, and the main use of ```BeautifulSoup``` is that it helps you parse HTML easily. \n",
    "\n",
    "**Note**: Discussion on HTML syntax is beyond the scope of this module, though even very basic HTML experience should be enough to understand web scraping. \n",
    "\n",
    "#### Use Case - Fetching Mobile App Reviews from Google Playstore\n",
    "\n",
    "Let's say you want to understand why people install and uninstall mobile apps, and why they like or dislike certain apps. A very rich source of app-reviews data is the Google Playstore, where people write their feedback about the app. \n",
    "\n",
    "The reviews of the Facebook messenger app can be found here: https://play.google.com/store/apps/details?id=com.facebook.orca&hl=en\n",
    "\n",
    "We will scrape reviews of the Messenger app, i.e. get them into python, and then you can do some interesting analyses on that.\n",
    "\n",
    "\n",
    "#### Parsing HTML Data using  BeautifulSoup and Requests\n",
    "\n",
    "To start using BeautifulSoup, install it using ```pip install beautifulsoup4```, and load the module bs4 using ```import bs4```. Also, install the requests module using ```pip install requests```.\n",
    "\n",
    "The general procedure to get data from websites is:\n",
    "1. Use ```requests``` to connect to a URL and get data from it\n",
    "2. Create a ```BeautifulSoup``` object \n",
    "3. Get attributes of the ```BeautifulSoup``` object (i.e. the HTML elements that you want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, bs4\n",
    "\n",
    "# getting HTML from the Google Play web page\n",
    "url = \"https://play.google.com/store/apps/details?id=com.facebook.orca&hl=en\"\n",
    "req = requests.get(url)\n",
    "\n",
    "# create a bs4 object\n",
    "# To avoid warnings, provide \"html5lib\" explicitly\n",
    "soup = bs4.BeautifulSoup(req.text, \"html5lib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a bs4 object, you can use it to get specific parts of the HTML document. \n",
    "\n",
    "In HTML, the most common elements are either a part of a ```class```, or are assigned an ```id```. A typical HTML code looks like this:\n",
    "\n",
    "\n",
    "```\n",
    "    <body>\n",
    "        <div class=\"some_class\">\n",
    "            <p id=\"a_unique_id\">\n",
    "                A paragraph that you can read on the webpage here.\n",
    "            </p>\n",
    "        </div>\n",
    "    </body>\n",
    "```\n",
    "\n",
    "\n",
    "The easiest way to get specific parts of the webpage is using the ```soup.select()``` method of bs4. For e.g.:\n",
    "- ```soup.select('div')``` selects all the elements inside a 'div' tag\n",
    "- ```soup.select('div > p')``` selects all the ```<p>``` elements within div tags\n",
    "- ```soup.select('.some_class')``` selects elements inside  ```class = \"some_class\"```\n",
    "- ```soup.select('#some_id')``` selects all the elements inside the ```id=\"some_id\"``` element \n",
    "\n",
    "\n",
    "Now, you need to find the ids / classes / divs etc. inside which the reviews are located. To find those, go to the webpage, right click, and choose 'Inspect'. It will open up the HTML code.\n",
    "\n",
    "We'll not go into details, but if you look carefully, all the reviews are located inside a ```<div class=\"review-body\"> ```. So we use that class to fetch the reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all the text inside class = \"review-body\"\n",
    "reviews = soup.select('.review-body')\n",
    "print(type(reviews))\n",
    "print(len(reviews))\n",
    "print(\"\\n\")\n",
    "\n",
    "# printing an element of the reviews list\n",
    "print(reviews[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that ```reviews``` is a list, and each element of the list contains some HTML code + the review string that we want.\n",
    "\n",
    "We still need to subset only the actual review string, and ignore all the other HTML, as shown in the highlighted section below (we want the bold string).\n",
    "\n",
    "```<div class=\"review-body with-review-wrapper\"> <span class=\"review-title\"></span>```\n",
    "**I love this app for it has many features but this is stressing me out bc it eats a lot of space and im only using samsung which is definitely cheap in giving internal storage. I have a lot of needs also, optimize the space of messenger. Thankyou. :) ily ppl who developed this. ** \n",
    "```<div class=\"review-link\" style=\"display:none\"> <a class=\"id-no-nav play-button tiny\" href=\"#\" target=\"_blank\"> Full Review </a> </div> </div>```\n",
    "\n",
    "\n",
    "Now, there are more than one ways to so this. One way is to ```str.split()``` the entire string into two parts using ```</span>``` as the separator (```str.split(\"sep\")``` separates a string into two parts at \"sep\").\n",
    "\n",
    "Then we'll get this:\n",
    "\n",
    "**I love this app for it has many features but this is stressing me out bc it eats a lot of space and im only using samsung which is definitely cheap in giving internal storage. I have a lot of needs also, optimize the space of messenger. Thankyou. :) ily ppl who developed this. ** \n",
    "```<div class=\"review-link\" style=\"display:none\"> <a class=\"id-no-nav play-button tiny\" href=\"#\" target=\"_blank\"> Full Review </a> </div> </div>```\n",
    "\n",
    "And then ```str.split()``` this string again using ```<div class=\"review-link'``` as the separator, so we'll get only the review string, i.e.:\n",
    "\n",
    "**I love this app for it has many features but this is stressing me out bc it eats a lot of space and im only using samsung which is definitely cheap in giving internal storage. I have a lot of needs also, optimize the space of messenger. Thankyou. :) ily ppl who developed this. ** \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: split using </span>\n",
    "r = reviews[6]\n",
    "print(type(r))\n",
    "\n",
    "# r is a bs4 tag, convert it into string to use str.split() \n",
    "part1 = str(r).split(\"</span>\")\n",
    "print(part1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now split part1 using '<div class=\"review-link' as the separator\n",
    "# and get the first element of the resulting list\n",
    "# we get the review\n",
    "part2 = part1[1].split('<div class=\"review-link')[0]\n",
    "part2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after applying ```str.split()``` twice, we get the review string.  Now, we simply apply this sequence of splitting to every element of the ```reviews``` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply this sequence of splitting to every element of the reviews list\n",
    "# using a list comprehension\n",
    "reviews_text = [str(r).split(\"</span>\")[1].split('<div class=\"review-link')[0] for r in reviews]\n",
    "\n",
    "# printing the first 10 reviews\n",
    "reviews_text[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now you can do some interesting analyses on the reviews, e.g. find how many people complain about memory, storage space, built-in camera etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Practice problem: Scraping amazon.in to get shoe price data \n",
    "import pprint\n",
    "\n",
    "url = \"https://www.amazon.in/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=sport+shoes\"\n",
    "req = requests.get(url)\n",
    "\n",
    "# create a bs4 object\n",
    "# To avoid warnings, provide \"html5lib\" explicitly\n",
    "soup = bs4.BeautifulSoup(req.text, \"html5lib\")\n",
    "\n",
    "# get shoe names\n",
    "# shoe_data = soup.select('.a-size-medium')\n",
    "# shoe_data = soup.select('.a-size-small.a-link-normal.a-text-normal')\n",
    "# print(len(shoe_data))\n",
    "# print(pprint.pprint(shoe_data))\n",
    "\n",
    "# get shoe prices\n",
    "shoe_prices = soup.select('.a-price-whole')\n",
    "print(len(shoe_prices))\n",
    "pprint.pprint(shoe_prices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
